# server.py
# Backend para Consult-IA
# - WebSocket /ws: recibe parciales/finales de voz a texto
# - Llama a OpenAI: (a) respuesta en streaming (tokens) y (b) JSON del formulario (schema)
# - Devuelve al cliente: assistant_token (stream), form_update (JSON), missing, suggestions
#
# Ejecutar:
#   setx OPENAI_API_KEY "tu_api_key"   (Windows, cerrar/reabrir terminal)
#   uvicorn server:app --host 0.0.0.0 --port 8001 --reload

import os, json, asyncio
from typing import Dict, Any, Optional, List
import logging
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, logger
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from dotenv import load_dotenv
from constants import SCHEMA, REQUIRED_KEYS

# SDK OpenAI nuevo (>=1.0)
from openai import OpenAI

# ------------------ Config ------------------

load_dotenv()  # lee .env si existe
logger = logging.getLogger("uvicorn.error")   # o: logging.getLogger(__name__)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
if not OPENAI_API_KEY:
    print("‚ö†Ô∏è  WARNING: OPENAI_API_KEY no est√° definido. Establ√©celo antes de usar el servidor.")

# Modelos: puedes subir a gpt-4o si deseas mejor razonamiento, o bajar a mini para costo/latencia
OPENAI_MODEL_TEXT = os.getenv("OPENAI_MODEL_TEXT", "gpt-4o-mini")   # texto en streaming
OPENAI_MODEL_JSON = os.getenv("OPENAI_MODEL_JSON", "gpt-4o-mini")   # structured outputs

client = OpenAI(api_key=OPENAI_API_KEY)

# Permite a tu front en http://localhost:4200 (ajusta para producci√≥n)
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:4200,http://127.0.0.1:4200").split(",")
FRONTEND_PATH = os.path.join(os.path.dirname(__file__), "../frontend/dist/consultia")

# ------------------ App ------------------

app = FastAPI(title="Consult-IA Backend", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[o.strip() for o in ALLOWED_ORIGINS],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Memoria simple por sesi√≥n (RAM)
sessions: Dict[str, Dict[str, Any]] = {}

# ------------------ Utilidades ------------------

def compute_missing(form: Dict[str, Any]) -> List[str]:
    """Eval√∫a campos m√≠nimos requeridos (ruta con puntos)."""
    missing: List[str] = []

    def exists(path: str) -> bool:
        parts = path.split(".")
        cur: Any = form
        for p in parts:
            if isinstance(cur, list):
                # lista no valida para seguir por clave
                return len(cur) > 0
            if not isinstance(cur, dict) or p not in cur:
                return False
            cur = cur[p]
        if cur is None:
            return False
        if isinstance(cur, str):
            return cur.strip() != ""
        if isinstance(cur, list):
            return len(cur) > 0
        return True

    for k in REQUIRED_KEYS:
        if not exists(k):
            missing.append(k)

    return missing

def build_suggestions(missing: List[str]) -> List[str]:
    tips_map = {
        "afiliacion.motivoConsulta": "Indique el motivo de consulta.",
        "anamnesis.sintomasPrincipales": "Mencione los s√≠ntomas principales.",
        "diagnosticos": "Registre al menos un diagn√≥stico (nombre, tipo y CIE‚Äë10 si es posible).",
        "tratamientos": "Consigne al menos un tratamiento (medicamento y dosis/indicaciones).",
    }
    return [tips_map[m] for m in missing if m in tips_map]

# ------------------ OpenAI helpers ------------------

async def stream_summary(ws: WebSocket, transcript: str):
    """Env√≠a respuesta de IA en streaming (token a token) a partir del transcript acumulado."""
    system = (
        "Eres un asistente cl√≠nico. Resume en 2‚Äì4 l√≠neas lo m√°s relevante del caso: "
        "motivo de consulta, s√≠ntomas clave, hallazgos, y si falta informaci√≥n para la historia cl√≠nica. "
        "No inventes datos y mant√©n tono profesional."
    )
    # notifica al frontend que reinicia el stream
    logger.info(f"[AI] stream_summary ENTER len={len(transcript)}") 

    await ws.send_json({"type": "assistant_reset"})

    logger.info("[AI] assistant_reset SENT")   
    # llamada con stream
    stream = client.chat.completions.create(
        model=OPENAI_MODEL_TEXT,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": transcript}
        ],
        temperature=0.2,
        stream=True
    )
    for chunk in stream:
        try:
            delta = chunk.choices[0].delta.get("content")
        except Exception:
            delta = None
        if delta:
            await ws.send_json({"type": "assistant_token", "delta": delta})

async def extract_form_patch(session_id: str, new_fragment: str) -> list[dict]:
    state = sessions[session_id]

    sys = (
        "Eres un asistente cl√≠nico. Tu tarea es mantener un objeto JSON de historia cl√≠nica actualizado.\n"
        "Se te dar√° el estado actual del formulario y un fragmento de transcripci√≥n.\n"
        "Devuelve SOLO un arreglo JSON con operaciones tipo JSON Patch (RFC6902).\n"
        "Cada operaci√≥n debe ser {op, path, value}.\n"
        "Usa paths estilo /afiliacion/nombreCompleto, /anamnesis/sintomasPrincipales/- para agregar.\n"
        "Si el fragmento no aporta informaci√≥n nueva, devuelve [].\n"
        "Devuelve SOLO JSON v√°lido, sin explicaciones."
    )

    user = {
        "current_form": state["json_state"],
        "new_fragment": new_fragment
    }

    resp = client.chat.completions.create(
        model=OPENAI_MODEL_JSON,
        messages=[
            {"role": "system", "content": sys},
            {"role": "user", "content": json.dumps(user, ensure_ascii=False)}
        ],
        temperature=0,
        response_format={"type": "json_object"}
    )

    content = resp.choices[0].message.content or "[]"
    try:
        patches = json.loads(content)
        if not isinstance(patches, list):
            patches = []
    except Exception:
        patches = []

    return patches

async def extract_form_incremental(session_id: str, new_fragment: str) -> dict:
    state = sessions[session_id]

    # append fragment as new user message
    state["messages"].append({
        "role": "user",
        "content": f"NUEVO FRAGMENTO: {new_fragment}. "
                   "Actualiza el objeto JSON en base a esto. "
                   "Si no hay cambios, devuelve el mismo JSON."
    })

    resp = client.chat.completions.create(
        model=OPENAI_MODEL_JSON,
        messages=state["messages"],
        temperature=0,
        response_format={"type": "json_object"}
    )

    content = resp.choices[0].message.content or "{}"
    updated_form = json.loads(content)

    # store the assistant's response in conversation for continuity
    state["messages"].append({"role": "assistant", "content": content})

    return updated_form

async def extract_form(transcript: str) -> dict:
    schema = SCHEMA

    sys = (
        "Eres un asistente cl√≠nico. Extrae SOLO los datos mencionados del transcript y "
        "devuelve EXCLUSIVAMENTE un objeto JSON v√°lido que siga EXACTAMENTE el siguiente JSON Schema. "
        "No inventes campos ni valores. Si algo no aparece, om√≠telo."
    )
    user = {
        "tarea": "Completar historia cl√≠nica desde el transcript.",
        "instrucciones": "Devuelve un UNICO objeto JSON que cumpla el schema.",
        "json_schema": schema,
        "transcript": transcript
    }

    resp = client.chat.completions.create(
        model=OPENAI_MODEL_JSON,
        messages=[
            {"role":"system","content": sys},
            {"role":"user","content": json.dumps(user)}  # üëà el mensaje contiene la palabra JSON y el schema
        ],
        temperature=0,
        response_format={"type":"json_object"}          # üëà ok, ya cumplimos el requisito
    )
    content = resp.choices[0].message.content or "{}"
    return json.loads(content)

# ------------------ Rutas ------------------

@app.get("/")
def root():
    return {"message": "TIMA Backend is running"}

@app.get("/health")
def health():
    ok = bool(OPENAI_API_KEY)
    return JSONResponse({"ok": ok, "model_text": OPENAI_MODEL_TEXT, "model_json": OPENAI_MODEL_JSON})

@app.websocket("/ws")
async def ws_endpoint(ws: WebSocket):
    await ws.accept()
    session_id = ws.query_params.get("session") or "default"
    state = sessions.setdefault(session_id, 
        {
            "final": "", 
            "partial": "", 
            # "last_form": {}, 
            # "json_state": {},
            "json_state": make_blank_from_schema(SCHEMA),
            "last_form": make_blank_from_schema(SCHEMA),
            "messages": []
        }
    )

    if not state["messages"]:  # first time
        state["messages"] = [
            {
                "role": "system",
                "content": (
                    "Eres un asistente cl√≠nico. Tu tarea es mantener un objeto JSON de historia cl√≠nica "
                    "actualizado en tiempo real. Devuelve SOLO JSON v√°lido y sigue EXACTAMENTE este schema: "
                    f"{json.dumps(SCHEMA, ensure_ascii=False)}"
                )
            },
            {
                "role": "assistant",
                "content": json.dumps(state["json_state"], ensure_ascii=False) or "{}"
            }
        ]

    try:
        while True:
            msg = await ws.receive_json()
            logger.info(f"[WS] recv: {msg}")   # <= NUEVO
            typ = msg.get("type")
            text = (msg.get("text") or "").strip()

            if typ == "partial":
                # solo mantener por si luego deseas usarlo
                state["partial"] = text

            elif typ == "final":
                if text:
                    # Concatena al buffer final con puntuaci√≥n simple
                    sep = "" if state["final"].endswith((" ", "\n", ".")) else " "
                    state["final"] = (state["final"] + sep + text + ". ").strip()
                    logger.info(f"[WS] final+= session={session_id} chunk_len={len(text)} total_chars={len(state['final'])}")

                    # 1) Streaming de asistente (still inline, so doc sees live summary)
                    try:
                        asyncio.create_task(stream_summary(ws, state["final"]))
                    except Exception as e:
                        logger.exception("[WS] stream_summary error")
                        await ws.send_json({"type": "error", "message": f"Stream error: {e}"})

                    # 2) Form extraction (run in background, don‚Äôt block loop)
                    # Fire background task
                    # asyncio.create_task(run_form_extraction(ws, session_id, state["final"], state.get("last_form", {})))
                    new_fragment = text
                    asyncio.create_task(
                        run_incremental_update(
                            ws,
                            session_id,
                            new_fragment,
                            state.get("json_state", {}),
                            state["final"]   # üîπ pass transcript explicitly
                        )
                    )

    except WebSocketDisconnect:
        # cliente cerrado
        return
    except Exception as e:
        try:
            await ws.send_json({"type": "error", "message": str(e)})
        except Exception:
            pass

async def run_incremental_update(
    ws: WebSocket,
    session_id: str,
    fragment: str,
    prev_form: dict,
    transcript: str
):
    try:
        # updated_form = await extract_form_incremental(prev_form, fragment)
        # updated_form = await extract_form_incremental(session_id, fragment)
        delta = await extract_form_delta(session_id, fragment)
        updated_form = deep_merge(prev_form, delta)
        missing = compute_missing(updated_form)
        suggestions = build_suggestions(missing)

        await ws.send_json({
            "type": "form_update",
            "form": updated_form,
            "missing": missing,
            "suggestions": suggestions
        })

        # Compute deltas vs previous form
        deltas = compute_deltas(prev_form, updated_form)
        if deltas:
            explained = await explain_deltas(transcript, deltas)
            await ws.send_json({"type": "form_delta", "changes": explained})

        # Update session state
        sessions[session_id]["json_state"] = updated_form
        sessions[session_id]["last_form"] = updated_form

    except Exception as e:
        logger.exception("[WS] incremental update error")
        await ws.send_json({"type": "error", "message": f"Update error: {e}"})

# helper: aplanar dict a rutas "a.b.c"
def deep_merge(old: dict, new: dict) -> dict:
    result = old.copy()
    for k, v in new.items():
        if isinstance(v, dict) and isinstance(result.get(k), dict):
            result[k] = deep_merge(result[k], v)
        else:
            result[k] = v
    return result

async def run_form_extraction(ws: WebSocket, session_id: str, transcript: str, prev_form: dict):
    try:
        form = await extract_form(transcript)
        missing = compute_missing(form)
        suggestions = build_suggestions(missing)

        await ws.send_json({
            "type": "form_update",
            "form": form,
            "missing": missing,
            "suggestions": suggestions
        })

        # Compute deltas vs previous form
        deltas = compute_deltas(prev_form, form)
        if deltas:
            explained = await explain_deltas(transcript, deltas)
            await ws.send_json({"type": "form_delta", "changes": explained})

            sintomas = [c for c in explained if c["path"].startswith("anamnesis.sintomasPrincipales")]
            if sintomas:
                lista = []
                for c in sintomas:
                    v = c["value"]
                    if isinstance(v, list): lista += v
                    elif isinstance(v, str): lista.append(v)
                if lista:
                    await ws.send_json({
                        "type": "insight",
                        "label": "S√≠ntomas",
                        "text": "; ".join(dict.fromkeys(map(str, lista)))
                    })

        # update session state
        sessions[session_id]["last_form"] = form

    except Exception as e:
        logger.exception("[WS] form extraction error")
        await ws.send_json({"type": "error", "message": f"Extraction error: {e}"})

async def extract_form_delta(session_id: str, new_fragment: str) -> dict:
    """
    Ask GPT to return only the minimal changes (delta JSON), not the full schema.
    Example output: {"afiliacion": {"nombreCompleto": "Jimena Olivares"}}
    """
    state = sessions[session_id]

    sys = (
        "Eres un asistente que actualiza un objeto JSON de acuerdo a un schema dado.\n\n"

        "Entradas:\n"
        "  ‚Ä¢ El estado actual del objeto JSON.\n"
        "  ‚Ä¢ Un fragmento de texto.\n"
        "  ‚Ä¢ El JSON Schema completo, con descripciones de cada campo.\n\n"

        "Instrucciones:\n"
        "1. Devuelve SOLO un objeto JSON parcial con los campos que deben actualizarse basados en el fragmento.\n"
        "   - Si no hay informaci√≥n nueva, devuelve {}.\n"
        "2. Usa √∫nicamente claves y estructuras que existan en el schema.\n"
        "3. Lee las descripciones del schema para decidir el campo m√°s adecuado. Si varios campos son relevantes, actualiza m√°s de uno.\n"
        "4. Respeta los tipos de datos definidos en el schema (string, number, array, object, enum, etc.).\n"
        "5. Para arrays, agrega elementos en una lista.\n"
        "6. No inventes claves ni devuelvas texto adicional fuera del JSON.\n"
    )

    user = {
        "current_form": state["json_state"],
        "new_fragment": new_fragment
    }

    resp = client.chat.completions.create(
        model=OPENAI_MODEL_JSON,
        messages=[
            {"role": "system", "content": sys},
            {"role": "user", "content": json.dumps(user, ensure_ascii=False)}
        ],
        temperature=0,
        response_format={"type": "json_object"}
    )

    content = resp.choices[0].message.content or "{}"
    try:
        delta = json.loads(content)
        if not isinstance(delta, dict):
            delta = {}
    except Exception:
        delta = {}

    return delta

def _flatten(d, prefix=""):
    out = {}
    if isinstance(d, dict):
        for k, v in d.items():
            out.update(_flatten(v, f"{prefix}.{k}" if prefix else k))
    elif isinstance(d, list):
        out[prefix] = d
    else:
        out[prefix] = d
    return out

def compute_deltas(prev: dict, curr: dict):
    p = _flatten(prev); c = _flatten(curr)
    changes = []
    for path, val in c.items():
        if path not in p or p[path] != val:
            changes.append({"path": path, "value": val})
    return changes

async def explain_deltas(transcript: str, changes: list[dict]) -> list[dict]:
    """
    Devuelve una lista: [{path, value, reason, evidence}]
    Usa response_format=json_object (cumpliendo el requisito de mencionar JSON).
    Si falla, devuelve los cambios con reason/evidence vac√≠os.
    """
    if not changes:
        return []

    # Limitar transcript para evitar prompts gigantes
    transcript_trim = transcript[-4000:]

    system_msg = (
        "Eres un asistente cl√≠nico. Para cada cambio de la historia cl√≠nica, "
        "devuelve SOLO un objeto JSON v√°lido con la forma: "
        '{"explanations":[{"path":"...","reason":"...","evidence":"..."}]}. '
        "La propiedad 'reason' debe ser breve (<= 18 palabras). "
        "La propiedad 'evidence' debe ser una cita textual corta (<= 15 palabras) tomada del transcript "
        "que respalde el valor; si no hay una evidencia textual clara, deja evidence como cadena vac√≠a. "
        "No incluyas texto adicional fuera del objeto JSON."
    )

    user_payload = {
        "tarea": "Explicar por qu√© se a√±adi√≥/actualiz√≥ cada campo del formulario.",
        "instrucciones": "Responde con UN UNICO objeto JSON bajo la clave 'explanations'.",
        "transcript": transcript_trim,
        "changes": changes
    }

    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL_JSON,          # usa el mismo que en extract_form
            messages=[
                {"role": "system", "content": system_msg},
                # üëá el contenido incluye JSON (cumple el requisito)
                {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
            ],
            temperature=0,
            response_format={"type": "json_object"},
        )
        content = resp.choices[0].message.content or "{}"
        data = json.loads(content)
        by_path = {e.get("path"): e for e in (data.get("explanations") or []) if isinstance(e, dict) and e.get("path")}

        explained = []
        for ch in changes:
            e = by_path.get(ch["path"], {})
            explained.append({
                "path": ch["path"],
                "value": ch.get("value"),
                "reason": (e.get("reason") or "").strip(),
                "evidence": (e.get("evidence") or "").strip(),
            })
        return explained

    except Exception as ex:
        # Fallback silencioso: no trabar el flujo si la explicaci√≥n falla
        logger.warning("explain_deltas failed: %s", ex)
        return [{"path": ch["path"], "value": ch.get("value"), "reason": "", "evidence": ""} for ch in changes]
    
def make_blank_from_schema(schema: dict) -> Any:
    t = schema.get("type")
    if t == "object":
        return {k: make_blank_from_schema(v) for k, v in schema.get("properties", {}).items()}
    elif t == "array":
        return []
    else:
        return None

# ------------------ Main ------------------

if os.path.isdir(FRONTEND_PATH):
    app.mount("/", StaticFiles(directory=FRONTEND_PATH, html=True), name="frontend")

if __name__ == "__main__":
    # Permite: python server.py (√∫til en desarrollo)
    import uvicorn
    port = int(os.getenv("PORT", "8001"))
    uvicorn.run("server:app", host="0.0.0.0", port=port, reload=True)
